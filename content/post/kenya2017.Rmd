---
title: 'Opinion Polls and the Kenyan Presidential Election - 1'
author: "Inbok Rhee"
date: "2017-08-03"
lastmod: 2015-12-23
slug: "opinion-polls-and-the-kenyan-presidential-election-1"
tags: [ "kenya", "election", "opinion polls" ]
category:
  - "Kenya"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


It's five days to the election, and I've been playing a little bit with the opinion poll data for the 2017 Kenyan Elections. To the best of my abilities, I have collected the data from newspaper articles and the releases from major survey firms. I've only used surveys that were at least trying to be somewhat nationally repersentative, and those which included some basic details about the surveys including the total number of respondents or the survey period. This forced me to leave out the results from some pollsters like Trends and Insights For Africa (TIFA). There's also a lot of other caveats and limitations, but we can talk more about those later. 

## Trends: Narrowing the Gap

First things first, I've plotted the overtime trends in the polls, first by the level of supports for the two main candidates, Uhuru Kenyatta and Raila Odinga. Instead of picking the mid-date, I used all dates for which a given survey was conducted, to capture the durations for each surveys. To better capture the overall time trends, I fitted a lowess line for the two leading candidates. The resulting graph below shows how the gap between the two candidates closed down as we get closer to the elections.


```{r echo=FALSE, error=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=6}

library(MCMCpack)
library(lubridate)
library(reshape2)
library(extrafont)
library(ggplot2)
library(rvest)
library(ggrepel)
library(ggthemes)
library(scales)

# 1. Get Data --------------------------------------------------------------

surveys <- read.csv("/Users/idrhee/Dropbox/_doc/_archive/2017-2018/opinion_polls/2014_Seoul_Mayoral_Election_Analysis-master/kenya2017_fin.csv")
surveys$sample <- surveys$sample/surveys$days
surveys$day <- as.character(surveys$day)
surveys$day <- as.Date(surveys$day, "%Y%m%d")

surveys$firm[surveys$firm=="Star"] <- "Radio Africa Group"
surveys$firm1[surveys$firm1=="Star"] <- "Radio Africa Group"

# 2. Overtime Plot --------------------------------------------------------

surveys_new <- within(surveys, {
  N   <- sample #*(uhuru + raila)/100
  N_j <- N * uhuru /100
  N_p <- N * raila /100
  N_e <- sample * (100 - uhuru - raila)/100 
  dt  <- ymd(day)
})

library(ggrepel)
library(ggthemes)
library(RColorBrewer)
j_brew_colors <- brewer.pal(n = 8, name = "Set1")

surveys_new$dminus <- as.Date("2017-08-08")- as.Date(surveys_new$day, "%Y%m%d")
surveys_new$gap <- surveys_new$uhuru-surveys_new$raila


ggplot(melt(surveys_new, id.vars=c('dt', 'firm', 'firm1'), 
            measure.vars =c('uhuru', 'raila')), aes(dt, value)) + 
  geom_point(aes(shape=firm), alpha = 0.8, size=2) + 
  stat_smooth(aes(colour=variable), method = "loess") + 
  scale_y_continuous("Support (%)") + 
  scale_x_date(date_breaks = "1 month",  
               limits=as.Date(c("2016-03-01", "2017-08-01")),
               labels = date_format("%Y-%m")) + 
  theme_few() +
  labs(title = "2017: Candidate Support (%)",
       subtitle = "(Lowess fitting)",
       caption = "Source: Information from Various Polling Firms and News Media", 
       x = "Date", y = "Support (%)") +
  scale_color_manual(name  ="Candidates",
                     breaks=c("uhuru", "raila"),
                     labels=c("Uhuru", "Raila"),
                     values = c(j_brew_colors[1], j_brew_colors[5])) +
  scale_shape_discrete(solid=F, name  ="Polling Firms",
                       breaks=c("IPSOS", "Info Trak", "Radio Africa Group", "Zogby", "Centre for African Progress"),
                       labels=c("IPSOS", "InfoTrak", "Star/Radio Africa", "Zogby", "Centre for African Progress")) +  
  theme(axis.text.x = element_text(angle = 90, vjust=0.5, hjust=1))


```

The next graph contains the same information, but this thime using the difference in the rates of support. Again, here we can see that the gap is quickly coming down close to zero as we are getting close to the August 8th.


```{r echo=FALSE, error=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=6}
ggplot(surveys_new, aes(dt, gap)) +
  geom_point(aes(shape=firm), alpha = 0.5, size=2) +
  stat_smooth(method = "loess") +
  scale_x_date(date_breaks = "1 month", 
               limits=as.Date(c("2016-03-01", "2017-08-01")),
               labels = date_format("%Y-%m")) +
  theme_few() +
  labs(title = "2017: Uhuru Rupport - Raila Support",
       subtitle = "(Lowess fitting)",
       caption = "Source: Information from Various Polling Firms and News Media", 
       x = "Date", y = "Uhuru Support - Raila Support(%)") +
  scale_shape_discrete(solid=F, name  ="Polling Firms",
                       breaks=c("IPSOS", "Info Trak", "Radio Africa Group", "Zogby", "Centre for African Progress"),
                       labels=c("IPSOS", "InfoTrak", "Star/Radio Africa", "Zogby", "Centre for African Progress")) + 
  theme(axis.text.x = element_text(angle = 90, vjust=0.5, hjust=1)) 
```

## Simple Bayesian Prediction Model

But what can we learn from these polls other than the overtime trends as we get close to the election date? Borrowing from a similar analysis done for [the 2014 Seoul Mayoral Election](http://freesearch.pe.kr/archives/4086), I conducted a simple Bayesian Prediction Model with sequential learning. For those who are interested, here are some more details about the model used.

### Multinomial Likelihood with a Dirichlet Prior

* Support Counts ($j$: Uhuru , $p$: Raila, $e$: Others)
    - $n_j, n_p, n_e$
* Likelihood 
    - $X_j,X_p,X_e \sim Multinomial(n, \theta_{n_j}, \theta_{n_p}, \theta_{n_e})$ 
* Prior 
    - $\pi(\theta_j, \theta_p, \theta_e) \propto 1$ 
    - $\theta_{n_j}, \theta_{n_p}, \theta_{n_e} \sim Dirichlet(1,1,1)$ 
* Posterior 
    - $\theta_{n_j}, \theta_{n_p}, \theta_{n_e}|n_j,n_p,n_e \sim Dirichlet(n_j + 1, n_p + 1, n_e + 1)$


### Steps 

1. Set uniform prior. 
2. Update posterior distribution parameters on each survey. 
3. Do Monte Carlo Simulation and get samples on each parameters (100,000 samples). 
3. Get $\theta_p - \theta_j$ distribution and mean of that distribution.

The resulting distribution of the mean is plotted below. The mean of means retrieved is approximately 8.2% difference between Uhuru and Raila, with the 1st and 3rd quantiles being approximately 7.9% and 8.4% respectively. 

```{r echo=FALSE, error=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=6}
# 5. Bayesian Prediction --------------------------------------------------


library(ggmcmc, quietly = T)
library(plyr,quietly = T)

surveys_new <- surveys_new[order(surveys_new$dt,decreasing=F),]
surveys_new$N <- with(surveys_new, {N_j + N_p + N_e})


surveys_new_aggr <- ddply(surveys_new, "firm", summarize, N_j=sum(N_j), N=sum(N), N_p=sum(N_p), N_e=sum(N_e))

surveys_new <- surveys_new[!is.na(surveys_new$N),]

#uniform prior 
alpha <- c(1,1,1)
#alpha <- c(50.51,43.7)

baye_diffs <- c()
freq_diff <- c()
ci <- c()
#sequential learning  
for(i in 1:nrow(surveys_new)){
  obs <- unlist(surveys_new[i, c("N_j", "N_p", "N_e")])
  post <- MCmultinomdirichlet(obs, alpha, mc=100000)
  baye_diffs <- append(baye_diffs, round(mean(post[,1] - post[,2]), 3))
  alpha <- (alpha + obs)
  
  #samp <- rdirichlet(n, as.vector(alpha))
  #obs[1]/sum(obs) 
  #p <- obs[1]/sum(obs)
  #plus_minus <- qnorm(0.975) * sqrt( (obs[1]/sum(obs) * sum(obs[c(2,3)])/sum(obs))/(sum(obs) - 1))
  #print(sprintf("%f +- %f", p, plus_minus))
  
  p_1 <- obs[1]/sum(obs)
  p_2 <- obs[2]/sum(obs)
  conf_interval <- qnorm(0.975) * 1/sqrt(sum(obs)) * sqrt(p_1 *(1- p_1) + p_2 * (1 - p_2) + 2 * p_1 * p_2)
  freq_diff <- append(freq_diff, p_1 - p_2)
  ci <- append(ci, conf_interval)
}



diff_dist <- data.frame(diffs_val=as.numeric((post[,1] - post[,2]))*100)

mdiff <- mean(diff_dist$diffs_val)

error <- qt(0.975,df=length(diff_dist$diffs_val)-1)*sd(diff_dist$diffs_val)/sqrt(length(diff_dist$diffs_val))
left <- mean(diff_dist$diffs_val)-error
right <- mean(diff_dist$diffs_val)+error


ggplot(diff_dist, aes(diffs_val)) + 
  geom_histogram(binwidth=0.1) + 
  scale_x_continuous(breaks=round(c(as.vector(summary(diff_dist$diffs_val))), digits=3)) +
  theme_bw() +
  labs(title = "Expexted Difference Between Uhuru and Raila Support",
       subtitle = "Mean Posterior from Monte Carlo Simulation (100,000 times)",
       caption = "Source: Information from Various Polling Firms and News Media", 
       x = paste("Mean of Uhuru Support - Raila Support", 
                 "\n (Min, 1Q, Mean, 3Q, Max)"), y = "Number of Times")
```

## Caveats

So what does this mean? Well, at least this simple analysis show that the incumbent president may win by a much wider margin than what the most recent polls have predicted (see, e.g. [here](http://www.the-star.co.ke/news/2017/08/02/uhuru-raila-in-very-tight-race-three-polls-show_c1608626) and [here](https://www.standardmedia.co.ke/article/2001250012/ipsos-and-infotrak-differ-yet-again-on-who-will-win-presidential-election) for a summary of the most recent polls). The results from this model is also close to [Ken Opalo's more sophisticated model](https://kenopalo.com/2017/08/02/what-the-poll-numbers-tell-us-about-kenyas-presidential-election-next-tuesday/) which takes into account not only similar opinion polls data but also considers county-level turnout data and  puts Uhuru leading by 5.6% (model with less weight to registration rates) to 8% (more weight to registration rates).

With all that said, there are also a number of limitations to this model and the entire exercise. First, the overall number of polling data points may be too small compared to other advanced democracy contexts to make any accurate predictions. Whereas I have only been able to include a total of 20 pre-elections surveys, [Nate Silver's prediction for the 2016 U.S. Presidential Election](https://projects.fivethirtyeight.com/2016-election-forecast/), for example, utilized 12,624 polls at the national level alone and many more polling data at the state-level. Second, accuracy and integrity of (at least some) pollsters have been always a concern during the Kenyan election cycle. Even during this election cycle, there's been many news articles and op-eds questioning the trustworthiness of opinion polls based on factors ranging from the methodology used to who sponsored the polls in the first place (some example articles [here](http://www.nation.co.ke/news/politics/Why-did-media-ignore-poll-showing-Raila-ahead-of-Uhuru--/1064-4027776-u3xb9tz/index.html), [here]([here](http://www.capitalfm.co.ke/news/2017/07/how-useful-are-opinion-polls-experts-weigh-in/)), and [here](https://www.standardmedia.co.ke/ureport/story/2001248940/why-i-do-not-trust-both-ipsos-and-infotrak-polls) ). Third, even if we assume that the existing polls are accurately capturing the trends in voter prefrence, (suspected) [precedents of electoral regularities in the past](http://www.tandfonline.com/doi/full/10.1080/17531055.2013.871182), raises the possibility that the polling data could be disconnected with the actual voting records on the election day. Lastly, as in any election, there has been, and probably will be, quite a few dramatic events at the last minute. For instance, with just ten days to the election, the Deputy President William Ruto's house has been attacked by unknown assailant(s) minutes after he left the house ([link](http://www.nation.co.ke/news/Gunmen-attack-DP-Ruto-s-Sugoi-home/1056-4037082-t0ycchz/index.html)) and then with just a week left until the votes, the Independent Electoral and Boundaries Commission (IEBC)'s ICT Manager, Chris Msando, has been found dead with signs of torture ([link](http://www.nation.co.ke/news/IEBC-ICT-manager-Chris-Musando-dead/1056-4039424-8r8f7v/index.html)). Some in the opposition has been quick to suggest that both events have been orchestrated by the ruling Jubilee party ([link](http://www.the-star.co.ke/news/2017/08/03/it-is-no-secret-jubilee-killed-jacob-juma-chris-msando-wetangula_c1609576)). These events, and any other incidents that may happen between now and the 8th could potentially affect public opinion in a drastic manner during this time when the release of any additional poll results are prohibited.

So there are many reasons why the prediction presented in this blog post may not be useful. We will see in five days time. In the meantime, it'll be interesting to look at data from any previous elections as well as polling data broken down at the sub-national level, by counties or even constituencies. More on these soon.
